import chromadb
import json
import boto3
import pickle
import gzip
import os
from datetime import datetime
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any
import openai

class InstagramVectorDBManager:
    def __init__(self, s3_bucket: str, openai_api_key: str):
        self.s3_client = boto3.client('s3')
        self.s3_bucket = s3_bucket
        self.embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
        openai.api_key = openai_api_key
        
        # ë¡œì»¬ ì„ì‹œ ë””ë ‰í† ë¦¬
        self.local_temp_dir = "/tmp/vector_dbs"
        os.makedirs(self.local_temp_dir, exist_ok=True)
    
    def process_instagram_data(self, json_data: Dict[str, Any]) -> str:
        """
        Instagram JSON ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ ë²¡í„°DB ì—…ë°ì´íŠ¸ ë° ìš”ì•½ ìƒì„±
        """
        account_id = json_data["accountId"]
        content_id = json_data["contentId"]
        content_url = json_data["contentUrl"]
        comments = json_data["comments"]
        crawled_at = json_data["crawledAt"]
        
        print(f"Processing account {account_id}, content {content_id}")
        
        # 1. ë²¡í„° DB ë¡œë“œ ë˜ëŠ” ìƒì„±
        collection = self.get_or_create_vectordb(account_id)
        
        # 2. ìƒˆ ëŒ“ê¸€ë“¤ì„ ë²¡í„° DBì— ì¶”ê°€
        self.add_comments_to_vectordb(collection, comments, content_id, content_url, crawled_at)
        
        # 3. ë²¡í„° DBë¥¼ S3ì— ì €ì¥
        self.save_vectordb_to_s3(account_id, collection)
        
        # 4. ê³„ì • ìš”ì•½ ìƒì„±
        summary = self.generate_account_summary(collection, account_id)
        
        return summary
    
    def get_or_create_vectordb(self, account_id: int):
        """
        ê³„ì •ì˜ ë²¡í„° DBë¥¼ ë¡œë“œí•˜ê±°ë‚˜ ìƒˆë¡œ ìƒì„±
        """
        s3_key = f"vector_dbs/account_{account_id}.gz"
        local_path = f"{self.local_temp_dir}/account_{account_id}"
        
        try:
            # S3ì—ì„œ ê¸°ì¡´ ë²¡í„° DB ë‹¤ìš´ë¡œë“œ
            print(f"Loading existing vector DB for account {account_id}")
            self.s3_client.download_file(self.s3_bucket, s3_key, f"{local_path}.gz")
            
            # ì••ì¶• í•´ì œ í›„ ë¡œë“œ
            with gzip.open(f"{local_path}.gz", 'rb') as f:
                db_data = pickle.load(f)
            
            # ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            client = chromadb.PersistentClient(path=local_path)
            collection = client.get_or_create_collection(
                name=f"account_{account_id}",
                metadata={"account_id": account_id}
            )
            
            # ê¸°ì¡´ ë°ì´í„° ë³µì›
            if db_data:
                collection.add(
                    ids=db_data['ids'],
                    embeddings=db_data['embeddings'],
                    documents=db_data['documents'],
                    metadatas=db_data['metadatas']
                )
            
            print(f"Loaded existing vector DB with {collection.count()} items")
            
        except Exception as e:
            # ë²¡í„° DBê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            print(f"Creating new vector DB for account {account_id}: {e}")
            client = chromadb.PersistentClient(path=local_path)
            collection = client.get_or_create_collection(
                name=f"account_{account_id}",
                metadata={"account_id": account_id}
            )
        
        return collection
    
    def add_comments_to_vectordb(self, collection, comments: List[Dict], 
                                content_id: int, content_url: str, crawled_at: str):
        """
        ìƒˆ ëŒ“ê¸€ë“¤ì„ ë²¡í„° DBì— ì¶”ê°€
        """
        if not comments:
            print("No comments to add")
            return
        
        # ì¤‘ë³µ í™•ì¸ì„ ìœ„í•´ ê¸°ì¡´ IDë“¤ ì¡°íšŒ
        try:
            existing_data = collection.get()
            existing_ids = set(existing_data['ids']) if existing_data['ids'] else set()
        except:
            existing_ids = set()
        
        new_comments = []
        new_embeddings = []
        new_metadatas = []
        new_ids = []
        
        for comment in comments:
            comment_id = f"content_{content_id}_comment_{comment['externalCommentId']}"
            
            # ì¤‘ë³µ ëŒ“ê¸€ ìŠ¤í‚µ
            if comment_id in existing_ids:
                print(f"Skipping duplicate comment: {comment_id}")
                continue
            
            # ëŒ“ê¸€ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            comment_text = comment['text'].strip()
            if len(comment_text) < 5:  # ë„ˆë¬´ ì§§ì€ ëŒ“ê¸€ ìŠ¤í‚µ
                continue
            
            new_comments.append(comment_text)
            new_metadatas.append({
                "content_id": content_id,
                "content_url": content_url,
                "author": comment['accountNickname'],
                "external_comment_id": comment['externalCommentId'],
                "likes_count": comment['likesCount'],
                "published_at": comment['publishedAt'],
                "crawled_at": crawled_at,
                "added_to_db_at": datetime.now().isoformat()
            })
            new_ids.append(comment_id)
        
        if not new_comments:
            print("No new comments to add")
            return
        
        # ì„ë² ë”© ìƒì„±
        print(f"Generating embeddings for {len(new_comments)} new comments")
        embeddings = self.embedding_model.encode(new_comments)
        new_embeddings = [emb.tolist() for emb in embeddings]
        
        # ë²¡í„° DBì— ì¶”ê°€
        collection.add(
            ids=new_ids,
            embeddings=new_embeddings,
            documents=new_comments,
            metadatas=new_metadatas
        )
        
        print(f"Added {len(new_comments)} new comments to vector DB")
    
    def save_vectordb_to_s3(self, account_id: int, collection):
        """
        ë²¡í„° DBë¥¼ S3ì— ì••ì¶• ì €ì¥
        """
        try:
            # ì»¬ë ‰ì…˜ ë°ì´í„° ì¶”ì¶œ
            data = collection.get()
            
            db_data = {
                'ids': data['ids'],
                'embeddings': data['embeddings'],
                'documents': data['documents'],
                'metadatas': data['metadatas'],
                'saved_at': datetime.now().isoformat(),
                'total_count': len(data['ids']) if data['ids'] else 0
            }
            
            # ì••ì¶•í•´ì„œ ë¡œì»¬ì— ì €ì¥
            local_gz_path = f"{self.local_temp_dir}/account_{account_id}.gz"
            with gzip.open(local_gz_path, 'wb') as f:
                pickle.dump(db_data, f, protocol=pickle.HIGHEST_PROTOCOL)
            
            # S3ì— ì—…ë¡œë“œ
            s3_key = f"vector_dbs/account_{account_id}.gz"
            self.s3_client.upload_file(local_gz_path, self.s3_bucket, s3_key)
            
            print(f"Saved vector DB to S3: {s3_key}")
            
            # ë¡œì»¬ ì„ì‹œ íŒŒì¼ ì •ë¦¬
            os.remove(local_gz_path)
            
        except Exception as e:
            print(f"Error saving vector DB to S3: {e}")
    
    def generate_account_summary(self, collection, account_id: int) -> str:
        """
        ëŒ“ê¸€ ë°ì´í„° íŠ¹ì„±ì— ë§ëŠ” ë‹¤ì¸µì  ë¶„ì„ìœ¼ë¡œ ê³„ì • ìš”ì•½ ìƒì„±
        """
        try:
            # ì „ì²´ ëŒ“ê¸€ ìˆ˜ í™•ì¸
            total_count = collection.count()
            
            if total_count < 10:
                return f"ê³„ì • {account_id}: ë¶„ì„í•˜ê¸°ì— ëŒ“ê¸€ì´ ë¶€ì¡±í•©ë‹ˆë‹¤ ({total_count}ê°œ)"
            
            # ì „ì²´ ëŒ“ê¸€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            all_data = collection.get()
            comments = all_data['documents']
            metadatas = all_data['metadatas']
            
            # ë°©ë²• 1: í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ ëŒ€í‘œ ëŒ“ê¸€ ì„ íƒ
            representative_comments = self.get_representative_comments(comments)
            
            # ë°©ë²• 2: ê°ì •/íŠ¹ì„± ê¸°ë°˜ ë¶„ë¥˜
            categorized_comments = self.categorize_comments(comments, metadatas)
            
            # ë°©ë²• 3: ì‹œê°„ ê¸°ë°˜ íŠ¸ë Œë“œ ë¶„ì„
            temporal_analysis = self.analyze_temporal_trends(comments, metadatas)
            
            # ìµœì¢… ìš”ì•½ìš© ëŒ“ê¸€ ì„ ë³„
            selected_comments = (
                representative_comments[:15] +  # ëŒ€í‘œ ëŒ“ê¸€
                categorized_comments['high_engagement'][:10] +  # ì¸ê¸° ëŒ“ê¸€
                categorized_comments['recent'][:10] +  # ìµœì‹  ëŒ“ê¸€
                categorized_comments['questions'][:5]  # ì§ˆë¬¸/ë¬¸ì˜
            )
            
            # ì¤‘ë³µ ì œê±°
            unique_comments = list(set(selected_comments))[:40]
            
            # í†µê³„ ì •ë³´ì™€ í•¨ê»˜ LLM ìš”ì•½ ìƒì„±
            summary_prompt = f"""
Instagram ê³„ì • {account_id}ì˜ ëŒ“ê¸€ ë¶„ì„ ê²°ê³¼:

ğŸ“Š í†µê³„ ìš”ì•½:
- ì´ ëŒ“ê¸€ ìˆ˜: {total_count}ê°œ
- í‰ê·  ì¢‹ì•„ìš”: {categorized_comments['avg_likes']:.1f}ê°œ
- ì£¼ìš” ì‹œê°„ëŒ€: {temporal_analysis['peak_hours']}
- í™œë™ ê¸°ê°„: {temporal_analysis['date_range']}

ğŸ’¬ ëŒ€í‘œì ì¸ ëŒ“ê¸€ë“¤:
{chr(10).join(unique_comments[:25])}

ğŸ“ˆ ì°¸ì—¬ë„ ë†’ì€ ëŒ“ê¸€:
{chr(10).join(categorized_comments['high_engagement'][:10])}

â“ ì§ˆë¬¸/ë¬¸ì˜ì‚¬í•­:
{chr(10).join(categorized_comments['questions'][:5])}

ìœ„ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ì´ Instagram ê³„ì •ì˜ íŠ¹ì„±ì„ ìš”ì•½í•´ì£¼ì„¸ìš”:
1. ì£¼ìš” ì½˜í…ì¸  íŠ¹ì„±ê³¼ íŒ”ë¡œì›Œ ê´€ì‹¬ì‚¬
2. íŒ”ë¡œì›Œ ì°¸ì—¬ íŒ¨í„´ê³¼ ë°˜ì‘
3. ì£¼ìš” í”¼ë“œë°± ë° ê°œì„ ì 

4-6ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
"""

            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[{"role": "user", "content": summary_prompt}],
                max_tokens=500,
                temperature=0.3
            )
            
            summary = response.choices[0].message.content
            
            # ìš”ì•½ì— ë©”íƒ€ ì •ë³´ ì¶”ê°€
            final_summary = f"""
=== Instagram ê³„ì • {account_id} ë¶„ì„ ìš”ì•½ ===
ì´ ëŒ“ê¸€ ìˆ˜: {total_count}ê°œ
ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

{summary}
"""
            
            print(f"Generated summary for account {account_id}")
            return final_summary
            
        except Exception as e:
            print(f"Error generating summary: {e}")
            return f"ê³„ì • {account_id} ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    def get_representative_comments(self, comments: List[str]) -> List[str]:
        """í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ëŒ€í‘œì ì¸ ëŒ“ê¸€ë“¤ ì„ íƒ"""
        from sklearn.cluster import KMeans
        import numpy as np
        
        if len(comments) < 10:
            return comments
        
        # ì„ë² ë”© ìƒì„±
        embeddings = self.embedding_model.encode(comments)
        
        # í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ ë™ì  ê²°ì • (ëŒ“ê¸€ ìˆ˜ì— ë”°ë¼)
        n_clusters = min(8, max(3, len(comments) // 20))
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = kmeans.fit_predict(embeddings)
        
        # ê° í´ëŸ¬ìŠ¤í„°ì—ì„œ ì¤‘ì‹¬ì— ê°€ì¥ ê°€ê¹Œìš´ ëŒ“ê¸€ ì„ íƒ
        representative = []
        for i in range(n_clusters):
            cluster_indices = np.where(cluster_labels == i)[0]
            if len(cluster_indices) == 0:
                continue
            
            cluster_embeddings = embeddings[cluster_indices]
            cluster_center = kmeans.cluster_centers_[i]
            
            # ì¤‘ì‹¬ê³¼ ê°€ì¥ ê°€ê¹Œìš´ ëŒ“ê¸€ ì°¾ê¸°
            distances = np.linalg.norm(cluster_embeddings - cluster_center, axis=1)
            closest_idx = cluster_indices[np.argmin(distances)]
            representative.append(comments[closest_idx])
        
        return representative
    
    def categorize_comments(self, comments: List[str], metadatas: List[Dict]) -> Dict:
        """ëŒ“ê¸€ì„ íŠ¹ì„±ë³„ë¡œ ë¶„ë¥˜"""
        import re
        from datetime import datetime
        
        high_engagement = []  # ì¢‹ì•„ìš” ë§ì€ ëŒ“ê¸€
        questions = []        # ì§ˆë¬¸ ëŒ“ê¸€
        recent = []          # ìµœì‹  ëŒ“ê¸€
        total_likes = 0
        
        # ëŒ“ê¸€ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ í•¨ê»˜ ì²˜ë¦¬
        comment_data = list(zip(comments, metadatas))
        
        # ì¢‹ì•„ìš” ê¸°ì¤€ ì •ë ¬
        sorted_by_likes = sorted(comment_data, 
                               key=lambda x: x[1].get('likes_count', 0), 
                               reverse=True)
        
        # ì‹œê°„ ê¸°ì¤€ ì •ë ¬  
        sorted_by_time = sorted(comment_data,
                              key=lambda x: x[1].get('published_at', ''),
                              reverse=True)
        
        for comment, metadata in comment_data:
            likes_count = metadata.get('likes_count', 0)
            total_likes += likes_count
            
            # ì¢‹ì•„ìš” ë§ì€ ëŒ“ê¸€ (ìƒìœ„ 30%)
            if likes_count > 0:
                high_engagement.append(comment)
            
            # ì§ˆë¬¸ íŒ¨í„´ ê°ì§€
            if any(pattern in comment for pattern in ['?', 'ï¼Ÿ', 'ê¶ê¸ˆ', 'ë¬¸ì˜', 'ì§ˆë¬¸', 'ì–´ë–»ê²Œ', 'ì™œ', 'ì–¸ì œ']):
                questions.append(comment)
        
        # ìµœì‹  ëŒ“ê¸€ (ìƒìœ„ 30%)
        recent = [comment for comment, _ in sorted_by_time[:len(comments)//3]]
        
        return {
            'high_engagement': high_engagement[:15],
            'questions': questions[:10], 
            'recent': recent[:15],
            'avg_likes': total_likes / len(comments) if comments else 0
        }
    
    def analyze_temporal_trends(self, comments: List[str], metadatas: List[Dict]) -> Dict:
        """ì‹œê°„ëŒ€ë³„ í™œë™ íŒ¨í„´ ë¶„ì„"""
        from collections import Counter
        from datetime import datetime
        
        hours = []
        dates = []
        
        for metadata in metadatas:
            published_at = metadata.get('published_at', '')
            if published_at:
                try:
                    dt = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
                    hours.append(dt.hour)
                    dates.append(dt.date())
                except:
                    continue
        
        if not hours:
            return {'peak_hours': 'ì •ë³´ ì—†ìŒ', 'date_range': 'ì •ë³´ ì—†ìŒ'}
        
        # ê°€ì¥ í™œë°œí•œ ì‹œê°„ëŒ€ ì°¾ê¸°
        hour_counts = Counter(hours)
        peak_hour = hour_counts.most_common(1)[0][0] if hour_counts else 0
        
        # í™œë™ ê¸°ê°„
        if dates:
            min_date = min(dates)
            max_date = max(dates)
            date_range = f"{min_date} ~ {max_date}"
        else:
            date_range = "ì •ë³´ ì—†ìŒ"
        
        return {
            'peak_hours': f"{peak_hour}ì‹œê²½",
            'date_range': date_range
        }

# ì‚¬ìš© ì˜ˆì‹œ
def main():
    # ì„¤ì •
    S3_BUCKET = "your-vectordb-bucket"
    OPENAI_API_KEY = "your-openai-api-key"
    
    # ë§¤ë‹ˆì € ì´ˆê¸°í™”
    manager = InstagramVectorDBManager(S3_BUCKET, OPENAI_API_KEY)
    
    # ì˜ˆì‹œ JSON ë°ì´í„°
    sample_data = {
        "accountId": 1,
        "contentId": 76,
        "contentUrl": "https://www.instagram.com/p/DMohJuePSL4/",
        "commentsCount": 2,
        "crawledAt": "2025-09-21T11:35:01.609756900Z",
        "comments": [
            {
                "accountNickname": "well.freedom",
                "externalCommentId": "18066047266946777",
                "text": "ì˜¤ ì™„ì „ ìœ ìš©í• ê±°ê°™ì•„ìš© ğŸ”¥ğŸ”¥",
                "likesCount": 2,
                "publishedAt": "2025-07-28T01:41:12.000Z"
            },
            {
                "accountNickname": "kunst_gyun",
                "externalCommentId": "18061851164464824",
                "text": "ë§Œë‘ë‹˜, ì•ˆë…•í•˜ì„¸ìš”!\ní˜¹ì‹œ ì´ë©”ì¼ í•œë²ˆ í™•ì¸ ê°€ëŠ¥í•˜ì‹¤ê¹Œìš”? @tiro.kr ê´€ë ¨ ê±´ìœ¼ë¡œ ì—°ë½ë“œë ¸ìŠµë‹ˆë‹¤!",
                "likesCount": 0,
                "publishedAt": "2025-08-08T06:02:00.000Z"
            }
        ]
    }
    
    # ì²˜ë¦¬ ì‹¤í–‰
    summary = manager.process_instagram_data(sample_data)
    print("\n" + "="*50)
    print("FINAL SUMMARY:")
    print(summary)

if __name__ == "__main__":
    main()
